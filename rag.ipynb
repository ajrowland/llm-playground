{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b4d511",
   "metadata": {},
   "source": [
    "# Example RAG solution\n",
    "\n",
    "Imports PDFs and uses ChromaDB as a vector store.  Creates a chain and runs some test prompts.\n",
    "\n",
    "## Imports and set up\n",
    "\n",
    "```\n",
    "source .venv/bin/activate\n",
    "\n",
    "pip install dotenv\n",
    "pip install langchain\n",
    "pip install langchain_openai\n",
    "pip install langchain_ollama\n",
    "pip install langchain_core\n",
    "pip install langchain_community\n",
    "pip install langchain_chroma\n",
    "pip ibstall operator\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093f7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, hashlib\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Require a key if using OpenAI.\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Select a MODEL.\n",
    "MODEL = \"llama3.2\"\n",
    "# MODEL = \"smollm:135m\"\n",
    "# MODEL = \"gemma3\"\n",
    "# MODEL = \"embeddinggemma\"\n",
    "\n",
    "# Set model/embeddings based on the MODEL selected.\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = OllamaLLM(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce9049",
   "metadata": {},
   "source": [
    "## Create persistent vector store from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_pdf_folder_safely(\n",
    "    folder_path: str = \"./pdfs\",\n",
    "    persist_dir: str = \"./chroma_db\",\n",
    "    collection_name: str = \"pdf_docs\",\n",
    "    embeddings=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Ingest all PDFs in a folder into Chroma safely.\n",
    "    - Initializes DB if missing\n",
    "    - Skips PDFs already ingested\n",
    "    - Deduplicates chunks\n",
    "    \"\"\"\n",
    "    if embeddings is None:\n",
    "        embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "    # Initialize or connect to collection\n",
    "    db_exists = os.path.exists(persist_dir)\n",
    "    if db_exists:\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_dir,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "    else:\n",
    "        vectorstore = None  # will create on first PDF\n",
    "\n",
    "    # Scan folder for PDFs\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        filepath = os.path.join(folder_path, pdf_file)\n",
    "\n",
    "        # Load PDF and split pages\n",
    "        loader = PyPDFLoader(filepath)\n",
    "        pages = loader.load_and_split()\n",
    "\n",
    "        # Add metadata and stable IDs\n",
    "        ids = []\n",
    "        for i, page in enumerate(pages):\n",
    "            page.metadata[\"source\"] = pdf_file\n",
    "            raw_id = f\"{pdf_file}-{i}-{page.page_content}\"\n",
    "            ids.append(hashlib.md5(raw_id.encode(\"utf-8\")).hexdigest())\n",
    "\n",
    "        if vectorstore is not None:\n",
    "            # Check if already ingested\n",
    "            existing_docs = vectorstore._collection.get(where={\"source\": pdf_file})\n",
    "            if existing_docs[\"ids\"]:\n",
    "                print(f\"ðŸ“Œ PDF '{pdf_file}' already exists â€” skipping.\")\n",
    "                continue\n",
    "            # Add new PDF\n",
    "            vectorstore.add_documents(pages, ids=ids)\n",
    "            print(f\"âœ… PDF '{pdf_file}' added to existing collection.\")\n",
    "        else:\n",
    "            # First PDF: create DB and collection\n",
    "            vectorstore = Chroma.from_documents(\n",
    "                documents=pages,\n",
    "                embedding=embeddings,\n",
    "                persist_directory=persist_dir,\n",
    "                collection_name=collection_name,\n",
    "                ids=ids\n",
    "            )\n",
    "            print(f\"âœ… PDF '{pdf_file}' added â€” database initialized.\")\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Load the PDF\n",
    "vectorstore = ingest_pdf_folder_safely()\n",
    "\n",
    "# Use retriever\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce26169",
   "metadata": {},
   "source": [
    "## Create and test a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c2995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template.\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't\n",
    "answer the question, replay \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# prompt.format(context=\"Here is some context\", question=\"Here is a question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ac64e",
   "metadata": {},
   "source": [
    "## Create chain, questions, and parse responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parser.\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Create chain.\n",
    "chain = (\n",
    "    {\n",
    "    \"context\": itemgetter(\"question\") | retriever,\n",
    "    \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# List questions to apply to prompt.\n",
    "questions = [\n",
    "   \"Who recommended Emma?\",\n",
    "   \"Who is Emma Rowland?\",\n",
    "   \"How many practitioners has she helped?\",\n",
    "   \"What is Reset Ready?\",\n",
    "   \"What is Spark Change Revolution?\",\n",
    "   \"Where should you start if you are new to business?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({ \"question\": question })}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
